{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e6e8847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lore_sa.dataset import TabularDataset\n",
    "from lore_sa.lore import TabularGeneticGeneratorLore\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from lore_sa.bbox import sklearn_classifier_bbox\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b90b910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentile Band Mapping (20% in each band):\n",
      "percentile_0: $0.1500 - $1.0720 (hundred thousands)\n",
      "percentile_1: $1.0720 - $1.5730 (hundred thousands)\n",
      "percentile_2: $1.5730 - $2.0940 (hundred thousands)\n",
      "percentile_3: $2.0940 - $2.9000 (hundred thousands)\n",
      "percentile_4: $2.9000 - $5.0000 (hundred thousands)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load and process the dataset\n",
    "dataset = fetch_california_housing()\n",
    "feature_names = list(dataset.feature_names)\n",
    "target_names = [f'percentile_{i}' for i in range(5)]\n",
    "\n",
    "prices = dataset.target\n",
    "\n",
    "# Create a DataFrame with features and target\n",
    "df = pd.DataFrame(dataset.data, columns=feature_names)\n",
    "\n",
    "# Use qcut to create 5 equal-sized bins (20% each)\n",
    "df['target'] = pd.qcut(prices, q=5, labels=target_names)\n",
    "\n",
    "# Get the bin edges for display\n",
    "_, bin_edges = pd.qcut(prices, q=5, retbins=True)\n",
    "print(\"\\nPercentile Band Mapping (20% in each band):\")\n",
    "for i in range(len(target_names)):\n",
    "    print(f\"{target_names[i]}: ${bin_edges[i]:.4f} - ${bin_edges[i+1]:.4f} (hundred thousands)\")\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('california_housing_5_split.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08de49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'target'\n",
    "dataset = TabularDataset.from_csv('california_housing_5_split.csv', class_name=target)\n",
    "dataset.df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9653b258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup',\n",
       "       'Latitude', 'Longitude', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "155b64ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': {'index': 8,\n",
       "  'distinct_values': ['percentile_4',\n",
       "   'percentile_3',\n",
       "   'percentile_2',\n",
       "   'percentile_1',\n",
       "   'percentile_0'],\n",
       "  'count': {'percentile_4': 4125,\n",
       "   'percentile_3': 4125,\n",
       "   'percentile_2': 4132,\n",
       "   'percentile_1': 4129,\n",
       "   'percentile_0': 4129}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.descriptor[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a9950a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numeric': {'MedInc': {'index': 0,\n",
       "   'min': 0.4999,\n",
       "   'max': 15.0001,\n",
       "   'mean': 3.8706710029069766,\n",
       "   'std': 1.8998217179452688,\n",
       "   'median': 3.5347999999999997,\n",
       "   'q1': 2.5633999999999997,\n",
       "   'q3': 4.74325},\n",
       "  'HouseAge': {'index': 1,\n",
       "   'min': 1.0,\n",
       "   'max': 52.0,\n",
       "   'mean': 28.639486434108527,\n",
       "   'std': 12.58555761211165,\n",
       "   'median': 29.0,\n",
       "   'q1': 18.0,\n",
       "   'q3': 37.0},\n",
       "  'AveRooms': {'index': 2,\n",
       "   'min': 0.8461538461538461,\n",
       "   'max': 141.9090909090909,\n",
       "   'mean': 5.428999742190376,\n",
       "   'std': 2.4741731394243187,\n",
       "   'median': 5.229128787878788,\n",
       "   'q1': 4.440716235896959,\n",
       "   'q3': 6.052380952380952},\n",
       "  'AveBedrms': {'index': 3,\n",
       "   'min': 0.3333333333333333,\n",
       "   'max': 34.06666666666667,\n",
       "   'mean': 1.096675149606208,\n",
       "   'std': 0.4739108567954661,\n",
       "   'median': 1.048780487804878,\n",
       "   'q1': 1.006079046038478,\n",
       "   'q3': 1.099526066350711},\n",
       "  'Population': {'index': 4,\n",
       "   'min': 3.0,\n",
       "   'max': 35682.0,\n",
       "   'mean': 1425.4767441860465,\n",
       "   'std': 1132.462121765341,\n",
       "   'median': 1166.0,\n",
       "   'q1': 787.0,\n",
       "   'q3': 1725.0},\n",
       "  'AveOccup': {'index': 5,\n",
       "   'min': 0.6923076923076923,\n",
       "   'max': 1243.3333333333333,\n",
       "   'mean': 3.0706551594363742,\n",
       "   'std': 10.386049562213618,\n",
       "   'median': 2.818115654360196,\n",
       "   'q1': 2.4297411475535755,\n",
       "   'q3': 3.2822609242736216},\n",
       "  'Latitude': {'index': 6,\n",
       "   'min': 32.54,\n",
       "   'max': 41.95,\n",
       "   'mean': 35.63186143410853,\n",
       "   'std': 2.1359523974571153,\n",
       "   'median': 34.26,\n",
       "   'q1': 33.93,\n",
       "   'q3': 37.71},\n",
       "  'Longitude': {'index': 7,\n",
       "   'min': -124.35,\n",
       "   'max': -114.31,\n",
       "   'mean': -119.56970445736432,\n",
       "   'std': 2.0035317235025882,\n",
       "   'median': -118.49,\n",
       "   'q1': -121.8,\n",
       "   'q3': -118.01}},\n",
       " 'categorical': {},\n",
       " 'ordinal': {},\n",
       " 'target': {'target': {'index': 8,\n",
       "   'distinct_values': ['percentile_4',\n",
       "    'percentile_3',\n",
       "    'percentile_2',\n",
       "    'percentile_1',\n",
       "    'percentile_0'],\n",
       "   'count': {'percentile_4': 4125,\n",
       "    'percentile_3': 4125,\n",
       "    'percentile_2': 4132,\n",
       "    'percentile_1': 4129,\n",
       "    'percentile_0': 4129}}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e05a8a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (20640, 9)\n",
      "Features retained: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude', 'target']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape: {dataset.df.shape}\")\n",
    "print(f\"Features retained: {list(dataset.df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c2f2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessor(dataset: TabularDataset):\n",
    "    \"\"\"Create preprocessing pipeline for numeric and categorical features.\"\"\"\n",
    "    numeric_indices = [v['index'] for v in dataset.descriptor['numeric'].values()]\n",
    "    categorical_indices = [v['index'] for v in dataset.descriptor['categorical'].values()]\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_indices),\n",
    "            ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical_indices)\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8901d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_hyperparameters(dataset: TabularDataset, sample_size=0.1, cv_folds=5, random_state=42):\n",
    "    \"\"\"Find best hyperparameters using a small subset of the data.\"\"\"\n",
    "    # Prepare full data\n",
    "    X = dataset.df.drop([target], axis=1)\n",
    "    y = dataset.df[target].values\n",
    "    \n",
    "    # Take 10% sample for hyperparameter search, stratified\n",
    "    X_sample, _, y_sample, _ = train_test_split(\n",
    "        X.values, y,\n",
    "        train_size=sample_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Create preprocessor\n",
    "    preprocessor = create_preprocessor(dataset)\n",
    "    \n",
    "    # Define parameter grid for RandomForestClassifier\n",
    "    param_grid = {\n",
    "        'randomforestclassifier__n_estimators': [50, 100, 200, 300],\n",
    "        'randomforestclassifier__max_depth': [10, 20, 30, None],\n",
    "        'randomforestclassifier__min_samples_split': [2, 5, 10],\n",
    "        'randomforestclassifier__min_samples_leaf': [1, 2, 4],\n",
    "        'randomforestclassifier__max_features': ['sqrt', 'log2'],\n",
    "        'randomforestclassifier__bootstrap': [True, False]\n",
    "    }\n",
    "    \n",
    "    # Create base pipeline\n",
    "    base_pipeline = make_pipeline(\n",
    "        preprocessor,\n",
    "        RandomForestClassifier(random_state=random_state, n_jobs=-1)\n",
    "    )\n",
    "    \n",
    "    print(\"GRID SEARCH FOR HYPERPARAMETER OPTIMIZATION (on 10% sample)\")\n",
    "    print(f\"Sample size: {len(X_sample)} instances ({sample_size*100}% of total)\")\n",
    "    print(f\"Total combinations to evaluate: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "    print(f\"Cross-validation folds: {cv_folds}\")\n",
    "    print(f\"Scoring metric: accuracy\\n\")\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=base_pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv_folds,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=4,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    print(\"Starting grid search on sample...\")\n",
    "    grid_search.fit(X_sample, y_sample)\n",
    "    print(\"\\nGrid search completed!\\n\")\n",
    "    \n",
    "    print(\"BEST HYPERPARAMETERS FOUND:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    print(f\"\\nBest CV Score on sample: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45dd2f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(dataset: TabularDataset, best_params, test_size=0.3, cv_folds=5, random_state=42):\n",
    "    \"\"\"Train final model on full dataset using best hyperparameters.\"\"\"\n",
    "    # Prepare data\n",
    "    X = dataset.df.drop([target], axis=1)\n",
    "    y = dataset.df[target].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X.values, y,\n",
    "        test_size=test_size, \n",
    "        random_state=random_state, \n",
    "        stratify=y  # Stratify on target\n",
    "    )\n",
    "    \n",
    "    # Create preprocessor\n",
    "    preprocessor = create_preprocessor(dataset)\n",
    "    \n",
    "    # Extract best parameters from grid search results\n",
    "    rf_params = {k.replace('randomforestclassifier__', ''): v \n",
    "                 for k, v in best_params.items()}\n",
    "    \n",
    "    # Create final pipeline with best parameters\n",
    "    final_pipeline = make_pipeline(\n",
    "        preprocessor,\n",
    "        RandomForestClassifier(**rf_params, random_state=random_state, n_jobs=-1)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTRAINING FINAL MODEL ON FULL DATASET\")\n",
    "    print(f\"Training set size: {len(X_train)} instances\")\n",
    "    print(f\"Test set size: {len(X_test)} instances\")\n",
    "    print(f\"Using best hyperparameters from grid search\\n\")\n",
    "    \n",
    "    final_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = final_pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    test_recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Cross-validation score on full training set\n",
    "    cv_scores = cross_val_score(final_pipeline, X_train, y_train, cv=cv_folds, scoring='accuracy')\n",
    "    \n",
    "    # Print results\n",
    "    print(\"FINAL MODEL RESULTS\")\n",
    "    print(f\"\\nCross-Validation Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print(f\"\\nTest Set Performance:\")\n",
    "    print(f\"  Accuracy:  {test_accuracy:.4f}\")\n",
    "    print(f\"  Precision: {test_precision:.4f}\")\n",
    "    print(f\"  Recall:    {test_recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {test_f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=dataset.descriptor[\"target\"][\"target\"][\"distinct_values\"]))\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'best_params': best_params,\n",
    "        'cv_mean_score': cv_scores.mean(),\n",
    "        'cv_std_score': cv_scores.std(),\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_precision': test_precision,\n",
    "        'test_recall': test_recall,\n",
    "        'test_f1': test_f1,\n",
    "        'classification_report': classification_report(y_test, y_pred, target_names=dataset.descriptor[\"target\"][\"target\"][\"distinct_values\"])\n",
    "    }\n",
    "    \n",
    "    # Convert to LORE-compatible bbox\n",
    "    bbox = sklearn_classifier_bbox.sklearnBBox(final_pipeline)\n",
    "    \n",
    "    return bbox, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08bed1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "runFilePath = \"DS User Case Model.pkl\"\n",
    "if not os.path.exists(runFilePath):\n",
    "    # Step 1: Find best hyperparameters on 10% sample\n",
    "    best_params = find_best_hyperparameters(\n",
    "        dataset=dataset,\n",
    "        sample_size=0.1,  # Use 10% of data for hyperparameter search\n",
    "        cv_folds=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Step 2: Train final model on full dataset with best parameters\n",
    "    bbox, results = train_final_model(\n",
    "        dataset=dataset,\n",
    "        best_params=best_params,\n",
    "        test_size=0.3,\n",
    "        cv_folds=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    with open(runFilePath, \"wb\") as f:\n",
    "        pickle.dump((bbox, results, best_params), f)\n",
    "\n",
    "with open(runFilePath, \"rb\") as f:\n",
    "    bbox, results, best_params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f36f315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'randomforestclassifier__bootstrap': True,\n",
       " 'randomforestclassifier__max_depth': 30,\n",
       " 'randomforestclassifier__max_features': 'sqrt',\n",
       " 'randomforestclassifier__min_samples_leaf': 1,\n",
       " 'randomforestclassifier__min_samples_split': 5,\n",
       " 'randomforestclassifier__n_estimators': 200}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a6c7994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance to explain:\n",
      "MedInc           8.301400\n",
      "HouseAge        21.000000\n",
      "AveRooms         6.238137\n",
      "AveBedrms        0.971880\n",
      "Population    2401.000000\n",
      "AveOccup         2.109842\n",
      "Latitude        37.860000\n",
      "Longitude     -122.220000\n",
      "Name: 1, dtype: float64\n",
      "Predicted class: percentile_4\n"
     ]
    }
   ],
   "source": [
    "x = dataset.df.drop([target], axis=1).iloc[1]\n",
    "print(f\"\\nInstance to explain:\")\n",
    "print(x)\n",
    "print(f\"Predicted class: {bbox.predict(x.values.reshape(1, -1))[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85f35e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabularLore = TabularGeneticGeneratorLore(bbox, dataset)\n",
    "# explanation_default = tabularLore.explain(x, num_instances=500)\n",
    "# explanation_big = tabularLore.explain(x, num_instances=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e80f30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabularLore.interactive_explanation(x, inJupyter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b6e9347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance to explain:\n",
      "MedInc           3.384100\n",
      "HouseAge        29.000000\n",
      "AveRooms         4.842031\n",
      "AveBedrms        1.002821\n",
      "Population    1919.000000\n",
      "AveOccup         2.706629\n",
      "Latitude        37.690000\n",
      "Longitude     -121.760000\n",
      "Name: 1000, dtype: float64\n",
      "Predicted class: percentile_2\n"
     ]
    }
   ],
   "source": [
    "x2 = dataset.df.drop([target], axis=1).iloc[1000]\n",
    "print(f\"\\nInstance to explain:\")\n",
    "print(x2)\n",
    "print(f\"Predicted class: {bbox.predict(x2.values.reshape(1, -1))[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052382b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [37632]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching LORE_sa explanation viz webapp\n",
      "==================================================\n",
      "Starting API server on 0.0.0.0:8000\n",
      "INFO:     127.0.0.1:49348 - \"GET /api/get-datasets HTTP/1.1\" 200 OK\n",
      "API server is ready at http://localhost:8000\n",
      "NPM version detected: 10.9.3\n",
      "Dependencies already installed, skipping npm install\n",
      "Application started successfully!\n",
      "API: http://localhost:8000/docs#/\n",
      "Client: http://localhost:8080\n",
      "==================================================\n",
      "Opening http://localhost:8080 in your default browser...\n",
      "Browser opened successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52953 - \"GET /api/check-custom-data HTTP/1.1\" 200 OK\n",
      "{'MedInc': 3.3841, 'HouseAge': 29.0, 'AveRooms': 4.842031029619182, 'AveBedrms': 1.002820874471086, 'Population': 1919.0, 'AveOccup': 2.706629055007052, 'Latitude': 37.69, 'Longitude': -121.76}\n",
      "INFO:     127.0.0.1:52953 - \"POST /api/explain HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52953 - \"GET /api/get-classes-colors?method=umap HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52953 - \"OPTIONS /api/update-visualization HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52953 - \"POST /api/update-visualization HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52953 - \"GET /api/get-classes-colors?method=pca HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63738 - \"POST /api/update-visualization HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63738 - \"GET /api/get-classes-colors?method=umap HTTP/1.1\" 200 OK\n",
      "{'MedInc': 3.3841, 'HouseAge': 29.0, 'AveRooms': 4.842031029619182, 'AveBedrms': 1.002820874471086, 'Population': 1919.0, 'AveOccup': 2.706629055007052, 'Latitude': 37.69, 'Longitude': -121.76}\n",
      "INFO:     127.0.0.1:62580 - \"POST /api/explain HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:62580 - \"GET /api/get-classes-colors?method=umap HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:64538 - \"POST /api/update-visualization HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:64538 - \"GET /api/get-classes-colors?method=pca HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "tabularLore.interactive_explanation(x2, inJupyter=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
