{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook will be to find the following data:\n",
    "* decision tree to plot in the visualization\n",
    "* the generated neighbourhood generated by LORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything will be tested on the iris dataset for quick running times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"iris.json\"\n",
    "# Load and prepare data\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = list(data.feature_names)\n",
    "target_names = list(data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LORE initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lore_sa.dataset import TabularDataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "        target  \n",
       "0       setosa  \n",
       "1       setosa  \n",
       "2       setosa  \n",
       "3       setosa  \n",
       "4       setosa  \n",
       "..         ...  \n",
       "145  virginica  \n",
       "146  virginica  \n",
       "147  virginica  \n",
       "148  virginica  \n",
       "149  virginica  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = {name: X[:, i] for i, name in enumerate(feature_names)}\n",
    "target_name = 'target'\n",
    "data_dict[target_name] = [target_names[i] for i in y]  # Map numerical targets to names\n",
    "\n",
    "dataset = TabularDataset.from_dict(data_dict, 'target')\n",
    "dataset.df.dropna(inplace = True)\n",
    "dataset.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lore_sa.bbox import sklearn_classifier_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model_generalized(dataset: TabularDataset, target_name:str, ):\n",
    "    numeric_indices = [v['index'] for k, v in dataset.descriptor['numeric'].items()]\n",
    "    categorical_indices = [v['index'] for k, v in dataset.descriptor['categorical'].items()]\n",
    "\n",
    "    # Create preprocessor using dynamic indices\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_indices),\n",
    "            ('cat', OrdinalEncoder(), categorical_indices)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Remove rare classes with fewer than 2 instances\n",
    "    valid_classes = dataset.df[target_name].value_counts()[dataset.df[target_name].value_counts() > 1].index\n",
    "    dataset.df = dataset.df[dataset.df[target_name].isin(valid_classes)]\n",
    "\n",
    "        # Select features and target\n",
    "    X = dataset.df.iloc[:, numeric_indices + categorical_indices]  # Select all features\n",
    "    y = dataset.df[target_name]\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    model = make_pipeline(preprocessor, RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return sklearn_classifier_bbox.sklearnBBox(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = train_model_generalized(dataset, target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numeric': {'sepal length (cm)': {'index': 0,\n",
       "   'min': 4.3,\n",
       "   'max': 7.9,\n",
       "   'mean': 5.843333333333334,\n",
       "   'std': 0.828066127977863,\n",
       "   'median': 5.8,\n",
       "   'q1': 5.1,\n",
       "   'q3': 6.4},\n",
       "  'sepal width (cm)': {'index': 1,\n",
       "   'min': 2.0,\n",
       "   'max': 4.4,\n",
       "   'mean': 3.0573333333333337,\n",
       "   'std': 0.4358662849366982,\n",
       "   'median': 3.0,\n",
       "   'q1': 2.8,\n",
       "   'q3': 3.3},\n",
       "  'petal length (cm)': {'index': 2,\n",
       "   'min': 1.0,\n",
       "   'max': 6.9,\n",
       "   'mean': 3.7580000000000005,\n",
       "   'std': 1.7652982332594662,\n",
       "   'median': 4.35,\n",
       "   'q1': 1.6,\n",
       "   'q3': 5.1},\n",
       "  'petal width (cm)': {'index': 3,\n",
       "   'min': 0.1,\n",
       "   'max': 2.5,\n",
       "   'mean': 1.1993333333333336,\n",
       "   'std': 0.7622376689603465,\n",
       "   'median': 1.3,\n",
       "   'q1': 0.3,\n",
       "   'q3': 1.8}},\n",
       " 'categorical': {},\n",
       " 'ordinal': {},\n",
       " 'target': {'target': {'index': 4,\n",
       "   'distinct_values': ['setosa', 'versicolor', 'virginica'],\n",
       "   'count': {'setosa': 50, 'versicolor': 50, 'virginica': 50}}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.descriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoding decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original value: [5.1 3.5 1.4 0.2]\n",
      "Encoded value: [[5.1 3.5 1.4 0.2]]\n",
      "Decoded value: [[5.1 3.5 1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "from lore_sa.encoder_decoder import ColumnTransformerEnc\n",
    "\n",
    "tabular_enc = ColumnTransformerEnc(dataset.descriptor)\n",
    "ref_value = dataset.df.iloc[0].values[:-1]\n",
    "encoded = tabular_enc.encode([ref_value])\n",
    "decoded = tabular_enc.decode(encoded)\n",
    "\n",
    "print(f\"Original value: {ref_value}\")\n",
    "print(f\"Encoded value: {encoded}\")\n",
    "print(f\"Decoded value: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the encoder is created using the dataset.descriptor, which is a dict so it can be saved to json and the encoder is then created based on the content read from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numeric': {'sepal length (cm)': {'index': 0,\n",
       "   'min': 4.3,\n",
       "   'max': 7.9,\n",
       "   'mean': 5.843333333333334,\n",
       "   'std': 0.828066127977863,\n",
       "   'median': 5.8,\n",
       "   'q1': 5.1,\n",
       "   'q3': 6.4},\n",
       "  'sepal width (cm)': {'index': 1,\n",
       "   'min': 2.0,\n",
       "   'max': 4.4,\n",
       "   'mean': 3.0573333333333337,\n",
       "   'std': 0.4358662849366982,\n",
       "   'median': 3.0,\n",
       "   'q1': 2.8,\n",
       "   'q3': 3.3},\n",
       "  'petal length (cm)': {'index': 2,\n",
       "   'min': 1.0,\n",
       "   'max': 6.9,\n",
       "   'mean': 3.7580000000000005,\n",
       "   'std': 1.7652982332594662,\n",
       "   'median': 4.35,\n",
       "   'q1': 1.6,\n",
       "   'q3': 5.1},\n",
       "  'petal width (cm)': {'index': 3,\n",
       "   'min': 0.1,\n",
       "   'max': 2.5,\n",
       "   'mean': 1.1993333333333336,\n",
       "   'std': 0.7622376689603465,\n",
       "   'median': 1.3,\n",
       "   'q1': 0.3,\n",
       "   'q3': 1.8}},\n",
       " 'categorical': {},\n",
       " 'ordinal': {},\n",
       " 'target': {'target': {'index': 4,\n",
       "   'distinct_values': ['setosa', 'versicolor', 'virginica'],\n",
       "   'count': {'setosa': 50, 'versicolor': 50, 'virginica': 50}}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"descriptor.json\", \"w\") as f:\n",
    "    json.dump(dataset.descriptor, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the value to use for the prediction can be found via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.1, 3.5, 1.4, 0.2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_value = dataset.df.iloc[0].values[:-1]\n",
    "list(ref_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and decoded using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.1, 3.5, 1.4, 0.2]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tabular_enc.encode([ref_value])\n",
    "decoded = tabular_enc.decode(encoded)\n",
    "list(decoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neighborhood generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lore_sa.neighgen import RandomGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select the istance to explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal length (cm)    5.1\n",
       "sepal width (cm)     3.5\n",
       "petal length (cm)    1.4\n",
       "petal width (cm)     0.2\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_row = 0\n",
    "x = dataset.df.iloc[num_row][:-1]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tabular_enc.encode([x.values])[0] # remove the class feature from the input instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creates the neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 1.3202377500938514],\n",
       "       [5.1, 3.5, 1.4, 1.3202377500938514],\n",
       "       [5.1, 3.5, 1.4, 1.5354793660377504],\n",
       "       [5.1, 3.5, 1.4, 1.5354793660377504],\n",
       "       [5.1, 3.5, 1.4, 1.5354793660377504],\n",
       "       [6.460748070744338, 3.5, 1.7967164675583747, 1.5354793660377504],\n",
       "       [6.460748070744338, 3.5, 1.7967164675583747, 1.5354793660377504],\n",
       "       [6.460748070744338, 3.5, 1.7967164675583747, 1.5354793660377504],\n",
       "       [6.460748070744338, 3.5, 1.7967164675583747, 1.5354793660377504],\n",
       "       [6.460748070744338, 3.5, 1.7967164675583747, 1.5354793660377504],\n",
       "       [6.460748070744338, 3.5, 1.7967164675583747, 1.5354793660377504],\n",
       "       [6.460748070744338, 3.5, 1.7967164675583747, 1.5354793660377504],\n",
       "       [6.460748070744338, 3.5, 1.7967164675583747, 1.5354793660377504],\n",
       "       [6.460748070744338, 3.5, 1.7967164675583747, 1.5354793660377504],\n",
       "       [6.460748070744338, 3.5, 1.7967164675583747, 2.0059886146876775],\n",
       "       [6.460748070744338, 3.452216509401693, 1.7967164675583747,\n",
       "        2.0059886146876775],\n",
       "       [7.539931626496525, 3.452216509401693, 1.7967164675583747,\n",
       "        2.0059886146876775],\n",
       "       [7.539931626496525, 3.452216509401693, 1.7967164675583747,\n",
       "        2.0059886146876775],\n",
       "       [7.539931626496525, 3.452216509401693, 1.7967164675583747,\n",
       "        2.0059886146876775],\n",
       "       [7.539931626496525, 3.452216509401693, 1.7967164675583747,\n",
       "        2.0059886146876775],\n",
       "       [5.726005160952531, 3.452216509401693, 1.7967164675583747,\n",
       "        2.0059886146876775],\n",
       "       [5.726005160952531, 3.452216509401693, 1.7967164675583747,\n",
       "        2.0059886146876775],\n",
       "       [5.726005160952531, 3.452216509401693, 1.7967164675583747,\n",
       "        2.0059886146876775],\n",
       "       [5.726005160952531, 3.452216509401693, 1.7967164675583747,\n",
       "        2.0059886146876775],\n",
       "       [5.726005160952531, 3.452216509401693, 3.9890942201011295,\n",
       "        2.0059886146876775],\n",
       "       [5.726005160952531, 3.452216509401693, 3.9890942201011295,\n",
       "        2.0059886146876775],\n",
       "       [5.726005160952531, 3.452216509401693, 3.9890942201011295,\n",
       "        2.0059886146876775],\n",
       "       [5.726005160952531, 3.452216509401693, 3.9890942201011295,\n",
       "        2.0059886146876775],\n",
       "       [5.726005160952531, 3.324722904719887, 3.9890942201011295,\n",
       "        2.0059886146876775],\n",
       "       [5.726005160952531, 3.324722904719887, 3.9890942201011295,\n",
       "        2.0059886146876775],\n",
       "       [5.0292630428206575, 3.324722904719887, 3.764098255752061,\n",
       "        2.0059886146876775],\n",
       "       [5.0292630428206575, 3.324722904719887, 3.764098255752061,\n",
       "        2.0059886146876775],\n",
       "       [5.0292630428206575, 3.324722904719887, 3.764098255752061,\n",
       "        2.0059886146876775],\n",
       "       [6.956780697320641, 3.324722904719887, 3.764098255752061,\n",
       "        2.0059886146876775],\n",
       "       [6.956780697320641, 3.324722904719887, 3.764098255752061,\n",
       "        2.0059886146876775],\n",
       "       [6.956780697320641, 2.8984791201901063, 4.498213636436176,\n",
       "        2.0059886146876775],\n",
       "       [6.956780697320641, 2.8984791201901063, 4.498213636436176,\n",
       "        2.0059886146876775],\n",
       "       [6.956780697320641, 2.8984791201901063, 4.498213636436176,\n",
       "        2.0059886146876775],\n",
       "       [6.956780697320641, 2.8984791201901063, 4.336342422601383,\n",
       "        2.0059886146876775],\n",
       "       [6.956780697320641, 2.8984791201901063, 4.336342422601383,\n",
       "        2.0059886146876775],\n",
       "       [6.956780697320641, 2.8984791201901063, 4.336342422601383,\n",
       "        2.0059886146876775],\n",
       "       [6.956780697320641, 2.8984791201901063, 4.336342422601383,\n",
       "        2.0059886146876775],\n",
       "       [6.956780697320641, 2.8984791201901063, 4.336342422601383,\n",
       "        2.0059886146876775],\n",
       "       [6.956780697320641, 2.8984791201901063, 4.336342422601383,\n",
       "        2.0059886146876775],\n",
       "       [6.956780697320641, 2.8984791201901063, 4.336342422601383,\n",
       "        2.0059886146876775],\n",
       "       [6.956780697320641, 2.8984791201901063, 4.336342422601383,\n",
       "        2.0059886146876775],\n",
       "       [6.956780697320641, 2.8984791201901063, 4.336342422601383,\n",
       "        2.0059886146876775],\n",
       "       [5.213959394166787, 2.8984791201901063, 4.336342422601383,\n",
       "        2.0059886146876775],\n",
       "       [5.213959394166787, 2.8984791201901063, 4.336342422601383,\n",
       "        2.0059886146876775],\n",
       "       [5.213959394166787, 2.8984791201901063, 4.336342422601383,\n",
       "        2.0059886146876775],\n",
       "       [5.213959394166787, 2.8984791201901063, 4.336342422601383,\n",
       "        2.0059886146876775],\n",
       "       [5.213959394166787, 2.8984791201901063, 4.336342422601383,\n",
       "        2.0059886146876775],\n",
       "       [5.213959394166787, 2.8984791201901063, 4.336342422601383,\n",
       "        2.0059886146876775],\n",
       "       [5.213959394166787, 2.8984791201901063, 3.887958224218381,\n",
       "        2.0059886146876775],\n",
       "       [5.213959394166787, 2.8984791201901063, 3.887958224218381,\n",
       "        2.0059886146876775],\n",
       "       [5.213959394166787, 3.5993552776703286, 3.887958224218381,\n",
       "        2.0059886146876775],\n",
       "       [5.213959394166787, 3.5993552776703286, 3.887958224218381,\n",
       "        2.0059886146876775],\n",
       "       [5.213959394166787, 3.5993552776703286, 3.887958224218381,\n",
       "        2.0059886146876775],\n",
       "       [5.213959394166787, 3.5993552776703286, 3.887958224218381,\n",
       "        2.0059886146876775],\n",
       "       [5.213959394166787, 3.5993552776703286, 3.887958224218381,\n",
       "        2.0059886146876775],\n",
       "       [5.213959394166787, 3.5993552776703286, 3.887958224218381,\n",
       "        2.0059886146876775],\n",
       "       [5.213959394166787, 3.5993552776703286, 6.669328650364743,\n",
       "        2.0059886146876775],\n",
       "       [5.213959394166787, 3.5993552776703286, 6.669328650364743,\n",
       "        1.332900040638042],\n",
       "       [5.213959394166787, 3.5993552776703286, 6.669328650364743,\n",
       "        1.332900040638042],\n",
       "       [5.213959394166787, 3.835150211609119, 6.669328650364743,\n",
       "        1.332900040638042],\n",
       "       [5.213959394166787, 3.6877378322777865, 6.669328650364743,\n",
       "        1.332900040638042],\n",
       "       [5.213959394166787, 3.6877378322777865, 6.669328650364743,\n",
       "        1.332900040638042],\n",
       "       [5.213959394166787, 3.6877378322777865, 5.902132442414232,\n",
       "        1.332900040638042],\n",
       "       [5.213959394166787, 3.6877378322777865, 5.902132442414232,\n",
       "        1.332900040638042],\n",
       "       [5.213959394166787, 3.6877378322777865, 5.435671821783353,\n",
       "        1.332900040638042],\n",
       "       [5.213959394166787, 3.6877378322777865, 5.435671821783353,\n",
       "        1.332900040638042],\n",
       "       [5.213959394166787, 3.6877378322777865, 2.0962228564509093,\n",
       "        1.332900040638042],\n",
       "       [5.213959394166787, 3.6877378322777865, 2.0962228564509093,\n",
       "        1.332900040638042],\n",
       "       [5.213959394166787, 2.0528772864393336, 2.0962228564509093,\n",
       "        1.332900040638042],\n",
       "       [5.213959394166787, 2.0528772864393336, 2.0962228564509093,\n",
       "        1.332900040638042],\n",
       "       [5.213959394166787, 2.0528772864393336, 2.0962228564509093,\n",
       "        1.332900040638042],\n",
       "       [5.213959394166787, 2.6146303784133456, 2.0962228564509093,\n",
       "        1.1233427385907722],\n",
       "       [5.213959394166787, 2.6146303784133456, 2.0962228564509093,\n",
       "        1.1233427385907722],\n",
       "       [5.213959394166787, 2.6146303784133456, 2.0962228564509093,\n",
       "        1.1233427385907722],\n",
       "       [6.079215033687625, 2.6146303784133456, 3.1881289509021467,\n",
       "        1.1233427385907722],\n",
       "       [6.079215033687625, 2.6146303784133456, 3.1881289509021467,\n",
       "        1.1233427385907722],\n",
       "       [6.079215033687625, 2.6146303784133456, 3.1881289509021467,\n",
       "        1.1233427385907722],\n",
       "       [6.079215033687625, 2.6146303784133456, 3.1881289509021467,\n",
       "        1.1233427385907722],\n",
       "       [6.079215033687625, 2.6146303784133456, 3.1881289509021467,\n",
       "        1.9591982240895343],\n",
       "       [6.079215033687625, 2.6146303784133456, 3.1881289509021467,\n",
       "        1.9591982240895343],\n",
       "       [6.079215033687625, 2.6146303784133456, 3.1881289509021467,\n",
       "        0.540101532518152],\n",
       "       [6.079215033687625, 2.6146303784133456, 3.1881289509021467,\n",
       "        0.540101532518152],\n",
       "       [6.079215033687625, 2.6146303784133456, 3.1881289509021467,\n",
       "        0.540101532518152],\n",
       "       [5.882845976643015, 2.6146303784133456, 3.1881289509021467,\n",
       "        0.540101532518152],\n",
       "       [5.882845976643015, 3.8687930587293025, 3.1881289509021467,\n",
       "        0.540101532518152],\n",
       "       [5.882845976643015, 3.8687930587293025, 3.1881289509021467,\n",
       "        0.540101532518152],\n",
       "       [5.882845976643015, 3.8687930587293025, 3.1881289509021467,\n",
       "        0.540101532518152],\n",
       "       [5.882845976643015, 3.8687930587293025, 3.1881289509021467,\n",
       "        2.2679646010912413],\n",
       "       [5.882845976643015, 3.8687930587293025, 3.1881289509021467,\n",
       "        2.2679646010912413],\n",
       "       [4.309538030764725, 2.25519011754023, 3.1881289509021467,\n",
       "        2.2679646010912413],\n",
       "       [4.309538030764725, 2.25519011754023, 3.1881289509021467,\n",
       "        2.2679646010912413],\n",
       "       [4.309538030764725, 2.25519011754023, 3.1881289509021467,\n",
       "        2.2679646010912413],\n",
       "       [5.026688888625247, 2.25519011754023, 3.1881289509021467,\n",
       "        2.2679646010912413],\n",
       "       [5.026688888625247, 2.25519011754023, 3.1881289509021467,\n",
       "        2.2679646010912413],\n",
       "       [5.026688888625247, 2.25519011754023, 3.1881289509021467,\n",
       "        2.2679646010912413]], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = RandomGenerator(bbox=bbox, dataset=dataset, encoder=tabular_enc, ocr=0.1)\n",
    "neighbour = gen.generate(z, 100, dataset.descriptor, tabular_enc)\n",
    "neighbour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### surrogate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lore_sa.surrogate import DecisionTreeSurrogate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create X, y and yz for the decision tree surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode the neighborhood to be labeled by the blackbox model\n",
    "neighb_train_X = tabular_enc.decode(neighbour)\n",
    "neighb_train_y = bbox.predict(neighb_train_X)\n",
    "# encode the target class to the surrogate model\n",
    "neighb_train_yz = tabular_enc.encode_target_class(neighb_train_y.reshape(-1, 1)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the surrogate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeSurrogate()\n",
    "x = dt.train(neighbour, neighb_train_yz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data extraction for the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the decision tree _tree for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.tree._tree.Tree at 0x2d67bb4c570>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.get_dt().tree_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from dataclasses import asdict\n",
    "\n",
    "@dataclass\n",
    "class TreeNode:\n",
    "    \"\"\"Class to store decision tree node information\"\"\"\n",
    "    # Unique identifier for the node in the tree\n",
    "    node_id: int\n",
    "    \n",
    "    # The name of the feature used for the decision at this node. \n",
    "    # If the node is a leaf, this will be `None`.\n",
    "    feature_name: Optional[str]\n",
    "    \n",
    "    # The threshold value for the feature used to split the data at this node. \n",
    "    # If the node is a leaf, this will be `None`.\n",
    "    threshold: Optional[float]\n",
    "    \n",
    "    # The node ID of the left child node. If the node is a leaf, this will be `None`.\n",
    "    left_child: Optional[int]\n",
    "    \n",
    "    # The node ID of the right child node. If the node is a leaf, this will be `None`.\n",
    "    right_child: Optional[int]\n",
    "    \n",
    "    # Indicates whether this node is a leaf node (`True` if leaf, `False` if internal).\n",
    "    is_leaf: bool\n",
    "    \n",
    "    # The class label predicted by the leaf node. \n",
    "    # Only set if the node is a leaf; otherwise, it is `None`.\n",
    "    class_label: Optional[str]\n",
    "    \n",
    "    # The number of samples (data points) that reached this node during training.\n",
    "    samples: int\n",
    "\n",
    "def extract_tree_structure(tree_classifier: DecisionTreeClassifier, feature_names: List[str], target_names: List[str]) -> List[TreeNode]: \n",
    "    \"\"\"\n",
    "    Extract node information from a trained DecisionTreeClassifier\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    tree_classifier : DecisionTreeClassifier\n",
    "        A trained sklearn DecisionTreeClassifier\n",
    "    feature_names : List[str]\n",
    "        A list of feature names\n",
    "    target_names : List[str]\n",
    "        A list of target class labels\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    List[TreeNode]\n",
    "        List of TreeNode objects containing the tree structure\n",
    "    \"\"\"\n",
    "    tree = tree_classifier.tree_\n",
    "    \n",
    "    nodes = []\n",
    "\n",
    "    for node_id in range(tree.node_count):\n",
    "        # Check if node is leaf\n",
    "        is_leaf = tree.children_left[node_id] == -1\n",
    "\n",
    "        # Get node information\n",
    "        if is_leaf:\n",
    "            # Get the class label based on the majority class in the leaf\n",
    "            class_label_index = int(tree.value[node_id].argmax())\n",
    "            class_label = target_names[class_label_index]\n",
    "            \n",
    "            node = TreeNode(\n",
    "                node_id=node_id,\n",
    "                feature_name=None,\n",
    "                threshold=None,\n",
    "                left_child=None,\n",
    "                right_child=None,\n",
    "                is_leaf=True,\n",
    "                class_label=class_label,\n",
    "                samples=int(tree.n_node_samples[node_id])\n",
    "            )\n",
    "        else:\n",
    "            feature_name = feature_names[int(tree.feature[node_id])]\n",
    "            threshold = float(tree.threshold[node_id])\n",
    "            left_child = int(tree.children_left[node_id])\n",
    "            right_child = int(tree.children_right[node_id])\n",
    "\n",
    "            node = TreeNode(\n",
    "                node_id=node_id,\n",
    "                feature_name=feature_name,\n",
    "                threshold=threshold,\n",
    "                left_child=left_child,\n",
    "                right_child=right_child,\n",
    "                is_leaf=False,\n",
    "                class_label=None,\n",
    "                samples=int(tree.n_node_samples[node_id])\n",
    "            )\n",
    "\n",
    "        nodes.append(node)\n",
    "\n",
    "    return nodes\n",
    "\n",
    "def generate_decision_tree_visualization_data(nodes):\n",
    "    \"\"\"\n",
    "    Save the tree structure to a JSON file\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nodes : List[TreeNode]\n",
    "        List of TreeNode objects to save\n",
    "    filename : str\n",
    "        Path to save the JSON file\n",
    "    indent : int\n",
    "        Number of spaces for indentation\n",
    "    \"\"\"\n",
    "    # Convert TreeNodes to dictionaries\n",
    "    nodes_dict = [asdict(node) for node in nodes]\n",
    "    \n",
    "    return nodes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"loreTreeTestTabular.json\", \"w\") as f:\n",
    "    json.dump(generate_decision_tree_visualization_data (\n",
    "        nodes= extract_tree_structure(\n",
    "            tree_classifier=dt.get_dt(),\n",
    "            feature_names=list(dataset.descriptor[\"numeric\"].keys()),\n",
    "            target_names=list(dataset.descriptor[\"target\"][\"target\"]['distinct_values'])\n",
    "        )\n",
    "    ), f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.get_dt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import Voronoi\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import unary_union\n",
    "import networkx as nx\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def preprocess_data(X):\n",
    "    \"\"\"\n",
    "    Standardize the data and apply PCA transformation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Input features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (transformed_data, pca_model, scaler_model)\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    pca = PCA(n_components=2)\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    return X_pca, pca, scaler\n",
    "\n",
    "def generate_decision_boundary_grid(X_pca, step=0.1):\n",
    "    \"\"\"\n",
    "    Generate a grid for decision boundary visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_pca : array-like\n",
    "        PCA transformed features\n",
    "    step : float, default=0.1\n",
    "        Step size for the grid\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (xx, yy) meshgrid arrays and (x_min, x_max, y_min, y_max) boundaries\n",
    "    \"\"\"\n",
    "    x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\n",
    "    y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, step),\n",
    "        np.arange(y_min, y_max, step)\n",
    "    )\n",
    "    return xx, yy, (x_min, x_max, y_min, y_max)\n",
    "\n",
    "def filter_points_by_class_kmeans(points, original_data, labels, threshold=500, threshold_multiplier=4, random_state=42):\n",
    "    \"\"\"\n",
    "    Filter points using K-means clustering to reduce data density, while preserving the original order.\n",
    "    \n",
    "    Instead of stacking by class (which reorders the data), we collect the original indices for each class,\n",
    "    perform the filtering, and then sort the indices to restore the original ordering.\n",
    "        \n",
    "    Parameters:\n",
    "    -----------\n",
    "    points : array-like\n",
    "        Input points to filter\n",
    "    original_data : array-like\n",
    "        Original feature data corresponding to points\n",
    "    labels : array-like\n",
    "        Class labels for points\n",
    "    threshold : int, default=500\n",
    "        Maximum number of points per class\n",
    "    threshold_multiplier : int, default=4\n",
    "        Multiplier for initial sampling size\n",
    "    random_state : int, default=42\n",
    "        Random state for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (filtered_points, filtered_original_data, filtered_labels)\n",
    "    \"\"\"\n",
    "    selected_indices = []\n",
    "    unique_classes = np.unique(labels)\n",
    "    \n",
    "    for cls in unique_classes:\n",
    "        # Get the indices in the original order for this class.\n",
    "        class_indices = np.where(labels == cls)[0]\n",
    "        class_points = points[class_indices]\n",
    "        n_points = len(class_points)\n",
    "        \n",
    "        if n_points > threshold:\n",
    "            sample_size = min(threshold * threshold_multiplier, n_points)\n",
    "            rng = np.random.RandomState(random_state)\n",
    "            sampled_indices_local = rng.choice(n_points, size=sample_size, replace=False)\n",
    "            sampled_points = class_points[sampled_indices_local]\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=threshold, random_state=random_state, n_init=10)\n",
    "            kmeans.fit(sampled_points)\n",
    "            centroids = kmeans.cluster_centers_\n",
    "            \n",
    "            # For each centroid, choose the point closest to it\n",
    "            for centroid in centroids:\n",
    "                distances = np.linalg.norm(class_points - centroid, axis=1)\n",
    "                closest_local_index = np.argmin(distances)\n",
    "                # Map back to the original index\n",
    "                selected_indices.append(class_indices[closest_local_index])\n",
    "        else:\n",
    "            selected_indices.extend(class_indices.tolist())\n",
    "    \n",
    "    # Sort the indices to maintain original order\n",
    "    selected_indices = np.sort(selected_indices)\n",
    "    \n",
    "    filtered_points = points[selected_indices]\n",
    "    filtered_original = original_data[selected_indices]\n",
    "    filtered_labels = labels[selected_indices]\n",
    "    return filtered_points, filtered_original, filtered_labels\n",
    "\n",
    "def create_voronoi_regions(xx, yy, Z, class_names):\n",
    "    \"\"\"\n",
    "    Create Voronoi regions for decision boundaries.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    xx : array-like\n",
    "        X coordinates of the grid\n",
    "    yy : array-like\n",
    "        Y coordinates of the grid\n",
    "    Z : array-like\n",
    "        Predicted classes for the grid points\n",
    "    class_names : list\n",
    "        List of class names corresponding to numeric indices\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (merged_regions, merged_classes) where merged_classes contains actual class names\n",
    "    \"\"\"\n",
    "    # Identify unique class indices present in Z\n",
    "    unique_z = np.unique(Z)\n",
    "    \n",
    "    # Create a mapping from original class indices to a new contiguous range starting from 0\n",
    "    # because sometimes some of the classes are missing from the generated neighborhood\n",
    "    index_map = {old_idx: new_idx for new_idx, old_idx in enumerate(sorted(unique_z))}\n",
    "    \n",
    "    # Apply the mapping to remap Z values\n",
    "    # This ensures the class indices are normalized (starting from 0 and sequential)\n",
    "    Z_remapped = np.array([index_map[z] for z in Z.ravel()]).reshape(Z.shape)\n",
    "    \n",
    "    # Initialize a graph for tracking adjacency of Voronoi regions\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Compute the Voronoi diagram from grid coordinates\n",
    "    vor = Voronoi(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Extract Voronoi regions and vertices\n",
    "    regions, vertices = vor.regions, vor.vertices\n",
    "    \n",
    "    # Dictionaries to store region information\n",
    "    region_class_map = {}  # Maps region index to class name\n",
    "    region_polygons = []    # Stores the actual polygonal regions\n",
    "    region_class_list = []  # Stores the class names corresponding to each region\n",
    "    region_index_map = {}   # Maps region index to polygon index\n",
    "    \n",
    "    polygon_idx = 0  # Index tracker for polygon storage\n",
    "    \n",
    "    # Iterate through Voronoi regions and assign class labels\n",
    "    for point_index, region_index in enumerate(vor.point_region):\n",
    "        region = regions[region_index]\n",
    "        \n",
    "        # Check for valid region (i.e., does not contain infinite points)\n",
    "        if not -1 in region and len(region) > 0:\n",
    "            # Create a polygon for the region using its vertex indices\n",
    "            polygon = Polygon([vertices[i] for i in region])\n",
    "            region_polygons.append(polygon)\n",
    "            \n",
    "            # Retrieve the remapped class index from Z_remapped\n",
    "            class_idx = int(Z_remapped.ravel()[point_index])\n",
    "            \n",
    "            # Ensure class index is within valid range\n",
    "            if class_idx < 0 or class_idx >= len(class_names):\n",
    "                raise ValueError(f\"Invalid remapped class index {class_idx} for class_names {class_names}\")\n",
    "            \n",
    "            # Map region index to class name\n",
    "            region_class_map[region_index] = class_names[class_idx]\n",
    "            region_class_list.append(class_names[class_idx])\n",
    "            \n",
    "            # Store mapping between Voronoi region index and our polygon index\n",
    "            region_index_map[region_index] = polygon_idx\n",
    "            \n",
    "            # Add region to graph\n",
    "            G.add_node(region_index)\n",
    "            \n",
    "            polygon_idx += 1\n",
    "    \n",
    "    # Identify adjacent regions with the same class and add edges to graph\n",
    "    for (p1, p2), ridge_vertices in zip(vor.ridge_points, vor.ridge_vertices):\n",
    "        if -1 in ridge_vertices:  # Ignore ridges extending to infinity\n",
    "            continue\n",
    "        \n",
    "        r1, r2 = vor.point_region[p1], vor.point_region[p2]\n",
    "        \n",
    "        # Only connect regions if they belong to the same class\n",
    "        if region_class_map.get(r1) == region_class_map.get(r2):\n",
    "            G.add_edge(r1, r2)\n",
    "    \n",
    "    # Merge connected regions with the same class\n",
    "    merged_regions = []\n",
    "    merged_classes = []\n",
    "    \n",
    "    for component in nx.connected_components(G):\n",
    "        # Merge all polygons in the connected component\n",
    "        merged_polygon = unary_union([\n",
    "            region_polygons[region_index_map[i]] \n",
    "            for i in component \n",
    "            if i in region_index_map\n",
    "        ])\n",
    "        merged_regions.append(merged_polygon)\n",
    "        \n",
    "        # Assign class based on the first region in the component\n",
    "        merged_classes.append(region_class_map[list(component)[0]])\n",
    "    \n",
    "    return merged_regions, merged_classes\n",
    "\n",
    "def format_pc_label(pc_loadings, feature_names, pc_index):\n",
    "    \"\"\"\n",
    "    Format the principal component label with feature contributions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pc_loadings : array-like\n",
    "        Principal component loadings\n",
    "    feature_names : list\n",
    "        List of feature names\n",
    "    pc_index : int\n",
    "        Index of the principal component\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Formatted label\n",
    "    \"\"\"\n",
    "    return f\"PC{pc_index + 1}: \" + \", \".join(\n",
    "        [f\"{name} ({value:+.2f})\" for name, value in zip(feature_names, pc_loadings)]\n",
    "    )\n",
    "\n",
    "def generate_pca_visualization_data(feature_names, X, y, pretrained_tree, class_names, step=0.1):\n",
    "    \"\"\"\n",
    "    Generate PCA visualization data and decision boundaries for a pre-trained decision tree.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_names : list\n",
    "        List of feature names\n",
    "    X : array-like\n",
    "        Input features\n",
    "    y : array-like\n",
    "        Target labels\n",
    "    pretrained_tree : DecisionTreeClassifier\n",
    "        Pre-trained decision tree classifier on original (non-PCA) data\n",
    "    step : float, default=0.1\n",
    "        Step size for decision boundary grid\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Visualization data including PCA coordinates, original data, and decision boundaries\n",
    "    \"\"\"\n",
    "    # Transform data\n",
    "    X_pca, pca, scaler = preprocess_data(X)\n",
    "    \n",
    "    # Generate grid in PCA space\n",
    "    xx, yy, (x_min, x_max, y_min, y_max) = generate_decision_boundary_grid(X_pca, step)\n",
    "    \n",
    "    # Transform grid points back to original space for prediction\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_original = pca.inverse_transform(grid_points)\n",
    "    grid_original = scaler.inverse_transform(grid_original)\n",
    "    \n",
    "    # Get predictions using the pre-trained tree\n",
    "    Z = pretrained_tree.predict(grid_original).reshape(xx.shape)\n",
    "    \n",
    "    # Filter PCA points and original data\n",
    "    filtered_pca_data, filtered_original_data, filtered_labels = filter_points_by_class_kmeans(\n",
    "        X_pca, X, y, threshold=2000, threshold_multiplier=5\n",
    "    )\n",
    "    \n",
    "    # Create Voronoi regions with class names\n",
    "    merged_regions, merged_classes = create_voronoi_regions(xx, yy, Z, class_names)\n",
    "    \n",
    "    # Format PC labels\n",
    "    pc1_label = format_pc_label(pca.components_[0], feature_names, 0)\n",
    "    pc2_label = format_pc_label(pca.components_[1], feature_names, 1)\n",
    "    \n",
    "    # Convert original data to lists of pd.Series\n",
    "    original_series_list = [\n",
    "        pd.Series(row, index=feature_names).to_dict()\n",
    "        for row in filtered_original_data\n",
    "    ]\n",
    "    \n",
    "    # Prepare visualization data\n",
    "    visualization_data = {\n",
    "        \"pcaData\": filtered_pca_data.tolist(),\n",
    "        \"originalData\": original_series_list,\n",
    "        \"targets\": filtered_labels.tolist(),\n",
    "        \"decisionBoundary\": {\n",
    "            \"regions\": [list(p.exterior.coords) for p in merged_regions],\n",
    "            \"regionClasses\": merged_classes,  # Now contains actual class names\n",
    "            \"xRange\": [float(x_min), float(x_max)],\n",
    "            \"yRange\": [float(y_min), float(y_max)],\n",
    "        },\n",
    "        \"xAxisLabel\": pc1_label,\n",
    "        \"yAxisLabel\": pc2_label,\n",
    "    }\n",
    "    \n",
    "    return visualization_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open (\"lorePCATestTabular.json\", \"w\") as f:\n",
    "    json.dump(generate_pca_visualization_data (\n",
    "        feature_names=feature_names, \n",
    "        class_names=dataset.descriptor[\"target\"][\"target\"]['distinct_values'], \n",
    "        X=neighb_train_X, \n",
    "        y=neighb_train_y, \n",
    "        pretrained_tree=dt.get_dt(), \n",
    "        step = 0.1\n",
    "    ), f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
