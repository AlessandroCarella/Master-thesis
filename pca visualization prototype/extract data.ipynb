{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"iris.json\"\n",
    "# Load and prepare data\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = list(data.feature_names)\n",
    "target_names = list(data.target_names)\n",
    "\n",
    "# PCA setup \n",
    "step = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_file = 'fishing.json'\n",
    "# df = pd.read_csv (\"test_dataset.csv\")\n",
    "# df.dropna(inplace=True)\n",
    "\n",
    "# target_names = [\n",
    "#     \"Poor Session\", \"Below Average\", \"Average Session\", \"Above Average\", \n",
    "#     \"Good Session\", \"Great Session\", \"Excellent Session\", \n",
    "#     \"Outstanding\", \"Legendary\", \"Epic\"\n",
    "# ]\n",
    "# feature_names = ['engine_age', 'length', 'power', 'month', 'weight', 'y_month',\n",
    "#        'year', 'surf_temp']\n",
    "\n",
    "# #remove non numerical data \n",
    "# # value adjusted for inflation (check dataset page for more info)\n",
    "# # other non relevant/not known features \n",
    "# df.drop([\"landing\", \"patch\", \"value_cpi\", \"y_\", \"ID\", \"dist\", \"patch_area\", \"weight_lym\", \"weight_lm\", \"val_lm\", \"val_lym\", \"nao_index\", \"price\"], axis=1, inplace=True)\n",
    "\n",
    "# y = df[\"value\"]\n",
    "# X = df[feature_names]\n",
    "\n",
    "# # Split the values into 10 categories with meaningful labels\n",
    "# y = pd.cut(y, bins=10, labels=target_names)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# y = label_encoder.fit_transform(y)  # Convert categories to numerical values\n",
    "\n",
    "# # PCA setup \n",
    "# step = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data and train the tree (following your original script)\n",
    "X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
    "dt_classifier_pca = DecisionTreeClassifier(random_state=42)  # Removed max_depth constraint\n",
    "dt_classifier_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "# Generate decision boundary data with finer grid\n",
    "x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\n",
    "y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, step), np.arange(y_min, y_max, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for the grid\n",
    "Z = dt_classifier_pca.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "def filter_points_by_class_kmeans(points, labels, threshold=500, thresholdMultiplierForClusteringSet=4, random_state=42):\n",
    "    \"\"\"\n",
    "    For each class, if the number of instances exceeds a given threshold,\n",
    "    cluster the points into k clusters and use the closest real points to the centroids\n",
    "    as representative points. Otherwise, keep all points.\n",
    "\n",
    "    Parameters:\n",
    "        points (np.ndarray): Array of shape (n_points, 2) with PCA coordinates.\n",
    "        labels (np.ndarray): Array of shape (n_points,) with class labels.\n",
    "        threshold (int): Minimum number of points a class must have before applying clustering.\n",
    "        thresholdMultiplierForClusteringSet (int): Multiplier for determining number of sampled points.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        filtered_points (np.ndarray): Filtered array of representative points.\n",
    "        filtered_labels (np.ndarray): Corresponding class labels.\n",
    "    \"\"\"\n",
    "    filtered_points = []\n",
    "    filtered_labels = []\n",
    "    unique_classes = np.unique(labels)\n",
    "\n",
    "    for cls in unique_classes:\n",
    "        # Get indices and points for this class\n",
    "        class_indices = np.where(labels == cls)[0]\n",
    "        class_points = points[class_indices]\n",
    "        n_points = len(class_points)\n",
    "\n",
    "        if n_points > threshold:\n",
    "            # Ensure we don't sample more than available\n",
    "            sample_size = min(threshold * thresholdMultiplierForClusteringSet, n_points)\n",
    "\n",
    "            rng = np.random.RandomState(random_state)\n",
    "            sampled_indices = rng.choice(n_points, size=sample_size, replace=False)\n",
    "            sampled_points = class_points[sampled_indices]\n",
    "\n",
    "            # Use k-means clustering to find cluster centers\n",
    "            kmeans = KMeans(n_clusters=threshold, random_state=random_state, n_init=10)\n",
    "            kmeans.fit(sampled_points)\n",
    "            centroids = kmeans.cluster_centers_\n",
    "\n",
    "            # Find the closest real point to each centroid\n",
    "            selected_points = []\n",
    "            for centroid in centroids:\n",
    "                distances = np.linalg.norm(class_points - centroid, axis=1)\n",
    "                closest_index = np.argmin(distances)\n",
    "                selected_points.append(class_points[closest_index])\n",
    "\n",
    "            selected_points = np.array(selected_points)\n",
    "        else:\n",
    "            # Keep original points if below threshold\n",
    "            selected_points = class_points\n",
    "\n",
    "        filtered_points.append(selected_points)\n",
    "        filtered_labels.extend([cls] * len(selected_points))\n",
    "\n",
    "    # Convert to numpy array\n",
    "    filtered_points = np.vstack(filtered_points)\n",
    "    filtered_labels = np.array(filtered_labels)\n",
    "\n",
    "    return filtered_points, filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original PCA points: 150\n",
      "Filtered PCA points: 150\n"
     ]
    }
   ],
   "source": [
    "filtered_pca_data, filtered_labels = filter_points_by_class_kmeans(X_pca, y, threshold=2000, thresholdMultiplierForClusteringSet = 5, random_state=42)\n",
    "\n",
    "print(\"Original PCA points:\", len(X_pca))\n",
    "print(\"Filtered PCA points:\", len(filtered_pca_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format all features for axis labels\n",
    "def format_pc_label(pc_loadings, feature_names, pc_index):\n",
    "    # Format as string: \"PC1: feature1 (+0.62), feature2 (-0.43), ...\"\n",
    "    label = f\"PC{pc_index + 1}: \" + \", \".join([f\"{name} ({value:+.2f})\" for name, value in zip(feature_names, pc_loadings)])\n",
    "    return label\n",
    "\n",
    "# Generate full labels for PC1 and PC2\n",
    "pc1_label = format_pc_label(pca.components_[0], feature_names, 0)\n",
    "pc2_label = format_pc_label(pca.components_[1], feature_names, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import Voronoi\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import unary_union\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Create a graph to store adjacent Voronoi regions\n",
    "G = nx.Graph()\n",
    "\n",
    "# Build the Voronoi diagram\n",
    "vor = Voronoi(np.c_[xx.ravel(), yy.ravel()])\n",
    "regions, vertices = vor.regions, vor.vertices\n",
    "\n",
    "# Create a mapping of region indices to their classes\n",
    "region_class_map = {}\n",
    "region_polygons = []\n",
    "region_class_list = []\n",
    "region_index_map = {}  # Maps Voronoi region index to polygon list index\n",
    "\n",
    "polygon_idx = 0\n",
    "for point_index, region_index in enumerate(vor.point_region):\n",
    "    region = regions[region_index]\n",
    "    if not -1 in region and len(region) > 0:  # Ignore infinite regions\n",
    "        polygon = Polygon([vertices[i] for i in region])\n",
    "        region_polygons.append(polygon)\n",
    "        region_class_map[region_index] = Z.ravel()[point_index]\n",
    "        region_class_list.append(Z.ravel()[point_index])\n",
    "        region_index_map[region_index] = polygon_idx  # Store index\n",
    "        G.add_node(region_index)  # Add region as a graph node\n",
    "        polygon_idx += 1\n",
    "\n",
    "# Find adjacent regions using Voronoi ridges\n",
    "for (p1, p2), ridge_vertices in zip(vor.ridge_points, vor.ridge_vertices):\n",
    "    if -1 in ridge_vertices:\n",
    "        continue  # Ignore infinite regions\n",
    "    r1, r2 = vor.point_region[p1], vor.point_region[p2]\n",
    "    \n",
    "    # Merge if they belong to the same class\n",
    "    if region_class_map.get(r1) == region_class_map.get(r2):\n",
    "        G.add_edge(r1, r2)\n",
    "\n",
    "# Find connected components (groups of merged regions)\n",
    "merged_regions = []\n",
    "merged_classes = []\n",
    "\n",
    "for component in nx.connected_components(G):\n",
    "    merged_polygon = unary_union([region_polygons[region_index_map[i]] for i in component if i in region_index_map])\n",
    "    merged_regions.append(merged_polygon)\n",
    "    merged_classes.append(region_class_map[list(component)[0]])  # Assign class from any region\n",
    "\n",
    "# Convert merged regions back to JSON format\n",
    "merged_region_polygons = [list(p.exterior.coords) for p in merged_regions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data for D3 visualization\n",
    "with open(data_file, \"w\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"pcaData\": filtered_pca_data.tolist(),\n",
    "            \"targets\": filtered_labels.tolist(),\n",
    "            \"targetNames\": list(target_names),\n",
    "            \"decisionBoundary\": {\n",
    "                \"regions\": merged_region_polygons,\n",
    "                \"regionClasses\": [int(c) for c in merged_classes],  # Convert NumPy int32 to Python int\n",
    "                \"xRange\": [float(x_min), float(x_max)],\n",
    "                \"yRange\": [float(y_min), float(y_max)],\n",
    "            },\n",
    "            \"xAxisLabel\": pc1_label,\n",
    "            \"yAxisLabel\": pc2_label,\n",
    "        },\n",
    "        f,\n",
    "        indent=4\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
