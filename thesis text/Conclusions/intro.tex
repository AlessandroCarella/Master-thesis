

This thesis has presented the design, development, and evaluation of an interactive visual interface for eXplainable AI methods that generate synthetic neighborhoods and employ decision tree surrogate models. The research addresses a fundamental challenge in the field of eXplainable Artificial Intelligence, which is how to transform complex algorithmic outputs consisting of synthetic data and logical rules into intuitive, explorable visual representations that support human understanding and trust in machine learning explanations.

This concluding chapter addresses three objectives. Section \ref{sec:comparison_vxai} compares the developed system within the broader landscape of visual analytics tools for eXplainable AI by conducting a systematic comparison with the five reference tools surveyed in Section \ref{sec:Visualizations for XAI}. The comparison analyzes how the thesis system's design choices, particularly its focus on coordinating spatial neighborhood visualization with surrogate model exploration, relate to alternative approaches targeting different aspects of model explainability. This identifies both the unique contributions of the synthetic neighborhood paradigm and the complementary strengths of existing tools addressing explanation requirements.
Section \ref{sec:Visualizations for XAI} extends this positioning analysis by examining how the three proposed tree layout visualizations—Tree Layout, Rule and Counterfactual Rules Centered, and Rule Centered—relate to the broader landscape of decision tree visualization techniques surveyed in Section \ref{sec:Visualizations for XAI}. The analysis identifies both the adoption of proven design patterns from established approaches and distinctive contributions specifically-made for synthetic neighborhood analysis. This comparison clarifies how the thesis system balances adherence to established tree visualization principles against specialized design choices necessitated by the neighborhood-based explanation paradigm.
Section \ref{sec:future enanchements} examines potential future developments that could extend the system's capabilities and address limitations identified during the design and development processes. These enhancements include both technical improvements to visualization quality and interaction mechanisms, as well as conceptual extensions that would expand the framework's applicability to additional explanation methods and use case scenarios. Together, these sections provide a comprehensive assessment of the thesis contributions while establishing directions for continued research in visual analytics for eXplainable AI.