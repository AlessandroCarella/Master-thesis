The primary objective of this thesis is to conceptualize and implement an interactive visualization framework designed for the exploration and analysis of the synthetic neighborhood and surrogate model algorithmically generated.
Stable and Actionable LOcal
Rule-based Explanation method ($\text{LORE}_{sa}$) \cite{guidotti2022stable}, extending LORE \cite{guidotti2019lore}, was the chosen candidate for the making and testing of the thesis's product. The visualization tool aims to improve the understanding of the user, distancing itself from the library originally provided output regarding rules and counterfactual rules.
The interpretable representations are required by domain experts, data scientists, and end-users who seek to understand and confirm machine learning model decisions.

In the following Section \ref{sec:lore_sa}, we will undertake an examination of the mechanisms by which $\text{LORE}_{sa}$ generates the aforementioned synthetic neighborhood and constructs the interpretable surrogate model.
Furthermore, the implementation choices, design decisions, and representational frameworks that the current version of the library adopts for the representation, organization, and presentation of the generated explanations.

The output produced by $\text{LORE}_{sa}$ is fundamentally composed of two complementary components: the extracted rules and the corresponding counterfactual rules, other than the fidelity of the explanation and a feature importance array. The extracted rules are supposed to constitute the logical conditions that characterize the decision patterns and feature interactions that lead to the predicted outcome for the instance under examination. These rules provide an explanation framework, clarifying the specific combinations of feature values and thresholds that support the model's prediction. Contrarily, the counterfactual rules represent the logical negation or alternative conditions that would result in different model predictions, offering a perspective on the decision boundaries and helping users understand what changes would be necessary to alter the predicted outcome.

Building upon this foundation and the identified limitations in current approaches, Chapter \ref{cap:design} presents the new design proposal for the visualization of the $\text{LORE}_{sa}$ outputs. The proposed system will enable users to navigate between different levels of abstraction, from high-level overview representations of the entire explanation space to detailed, instance-specific rule visualizations that facilitate exploration and confirmation of the generated explanations.